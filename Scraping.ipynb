{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhDN0QurQhVz",
        "outputId": "6fe49b4a-f8c1-45d5-9819-f015f1670015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting scrape at https://veterinarypartner.vin.com/default.aspx?pId=19239&catId=102887\n",
            "Scraping completed! Data saved in cats_and_dogs_data.txt\n",
            "Total length of scraped data: 381221 characters\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import re\n",
        "\n",
        "# Define the file to store the scraped content\n",
        "output_file = \"cats_and_dogs_data.txt\"\n",
        "visited_urls = set()  # Set to store and check visited URLs to avoid duplicates\n",
        "base_domain = \"veterinarypartner.vin\"  # Adjust this to the domain you're targeting\n",
        "total_scraped_length = 0  # Counter to keep track of the total length of the scraped data\n",
        "\n",
        "# Function to check if a URL belongs to the relevant domain\n",
        "def is_relevant_url(url):\n",
        "    return urlparse(url).netloc.endswith(base_domain)\n",
        "\n",
        "# Function to clean content by removing dates and empty lines\n",
        "def clean_content(content):\n",
        "    # Remove dates in formats like MM/DD/YYYY or Month Day, Year\n",
        "    content = re.sub(r'\\b(?:\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|[A-Za-z]+\\s\\d{1,2},\\s\\d{4})\\b', '', content)\n",
        "\n",
        "    # Remove empty lines\n",
        "    content = '\\n'.join([line.strip() for line in content.split('\\n') if line.strip()])\n",
        "\n",
        "    return content\n",
        "\n",
        "# Function to scrape and filter useful content\n",
        "def scrape_page(url):\n",
        "    \"\"\"Scrapes the content of a given URL and writes filtered content to the file.\"\"\"\n",
        "    global total_scraped_length  # Use the global variable to track length\n",
        "\n",
        "    if url in visited_urls:\n",
        "        return  # Skip this URL if it was already visited\n",
        "\n",
        "    # Mark this URL as visited\n",
        "    visited_urls.add(url)\n",
        "\n",
        "    try:\n",
        "        # Fetch the page content\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Initialize flags to indicate whether we are inside the 'Cats' or 'Dogs' sections\n",
        "        inside_target_section = False\n",
        "        content = \"\"\n",
        "\n",
        "        # Loop through all <h2> tags and search for the target domains \"Cats\" or \"Dogs\"\n",
        "        for tag in soup.find_all(\"h2\"):\n",
        "            if \"Cats\" in tag.get_text():  # Look for the Cats section\n",
        "                inside_target_section = True  # Start scraping the Cats section\n",
        "                content += f\"Found target section: {tag.get_text()}\\n\\n\"\n",
        "            elif \"Dogs\" in tag.get_text():  # Look for the Dogs section\n",
        "                inside_target_section = True  # Start scraping the Dogs section\n",
        "                content += f\"Found target section: {tag.get_text()}\\n\\n\"\n",
        "\n",
        "            # If we are inside the target section, start collecting content\n",
        "            if inside_target_section:\n",
        "                # Extract content after the <h2> tag, check the next tags (paragraphs, divs, etc.)\n",
        "                if tag.name == 'h2' and \"Cats\" not in tag.get_text() and \"Dogs\" not in tag.get_text():\n",
        "                    break  # Stop scraping if we reach the next unrelated section\n",
        "\n",
        "                # Add the text from paragraphs or other content under the target section\n",
        "                for p in tag.find_all_next([\"p\", \"div\"]):  # Scrape <p> and <div> tags after the <h2>\n",
        "                    text = p.get_text(separator=\"\\n\").strip()\n",
        "                    if text:  # Only add non-empty content\n",
        "                        content += text + \"\\n\"\n",
        "\n",
        "        # Clean up the content: remove dates and empty lines\n",
        "        content = clean_content(content)\n",
        "\n",
        "        # If we have useful content, write it to the file\n",
        "        if content.strip():  # Only write if thereâ€™s actual content\n",
        "            with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
        "                f.write(f\"URL: {url}\\n\\n\")  # Record the page URL\n",
        "                f.write(content)          # Save the page content\n",
        "                f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")  # Separator for readability\n",
        "\n",
        "            # Update the total length of scraped data\n",
        "            total_scraped_length += len(content)  # Add the length of the content\n",
        "\n",
        "        # Find and recursively scrape all unique and relevant links on the page\n",
        "        for link in soup.find_all(\"a\", href=True):\n",
        "            full_url = urljoin(url, link[\"href\"])  # Convert to absolute URL\n",
        "\n",
        "            # Only follow relevant links (same domain) and skip advertisements or irrelevant URLs\n",
        "            if full_url not in visited_urls and is_relevant_url(full_url) and 'logout' not in full_url:\n",
        "                time.sleep(1)  # Add a delay to respect the server\n",
        "                print(f\"Following link: {full_url}\")\n",
        "                scrape_page(full_url)  # Recursively scrape the sublink\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "\n",
        "def main():\n",
        "    start_url = \"https://veterinarypartner.vin.com/default.aspx?pId=19239&catId=102887\"  # Example starting URL\n",
        "    print(f\"Starting scrape at {start_url}\")\n",
        "    scrape_page(start_url)\n",
        "    print(f\"Scraping completed! Data saved in {output_file}\")\n",
        "    print(f\"Total length of scraped data: {total_scraped_length} characters\")\n",
        "\n",
        "# Run the main function to begin scraping\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}